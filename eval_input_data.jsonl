{"query": "Can you explain how Cache Fusion contributes to reducing time-to-first-token (TTFT) compared to a full key-value recompute? Also, what is the reported reduction in TTFT achieved by CacheBlend?", "response": "Certainly! Cache Fusion is a technique aimed at improving the efficiency of distributed systems, particularly in reducing the time-to-first-token (TTFT) for serving requests.\n\n### Cache Fusion and TTFT Reduction\nCache Fusion works by enabling distributed systems to share cached data seamlessly across multiple nodes, avoiding the need to recompute values or fetch data from slower storage layers like databases. Here's how it helps reduce TTFT compared to a full key-value recompute:\n\n1. **Data Sharing Across Nodes**: With Cache Fusion, when a request is received by a node that does not have the required data in its local cache, the system can fetch the data from the cache of another node in the cluster rather than recomputing it or retrieving it from the original data source.\n\n2. **Avoiding Recompute Latency**: Recomputing a key-value pair can be computationally expensive and time-consuming, especially if it involves complex processing or querying a slow backend database. Cache Fusion avoids this by leveraging already-computed results stored in the cache of other nodes.\n\n3. **Optimized Communication**: Cache Fusion uses efficient communication protocols to transfer data between nodes, minimizing the overhead associated with these operations.\n\nBy using Cache Fusion, the system ensures that the data is served to the client quickly, often bypassing the need for expensive recompute operations.\n\n### CacheBlend and TTFT Reduction\nCacheBlend is an advanced caching mechanism that builds on techniques like Cache Fusion to further optimize TTFT. It introduces intelligent blending of caching and computation strategies, dynamically deciding whether to fetch cached data or recompute it based on factors like data freshness and access patterns.\n\nThe reported reduction in TTFT achieved by CacheBlend is **up to 50%**. This significant improvement comes from its ability to:\n- Minimize data recomputation by proactively sharing and utilizing cached results.\n- Reduce latency caused by cache misses through efficient cache management and data fusion techniques.\n\nBy combining these optimizations, CacheBlend effectively halves the time it takes to serve the first token of a response in many scenarios, making it a powerful tool for improving the performance of distributed systems.", "context": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill.", "ground_truth": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill."}
{"query": "What is the throughput increase achieved by CacheBlend compared to full KV recompute?", "response": "CacheBlend achieves a significant throughput increase compared to full key-value (KV) recompute. Specifically, it boosts throughput by approximately **9.4\u00d7**. This improvement is achieved by leveraging a blend of caching strategies and computation to optimize performance in data-intensive workloads.", "context": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill.", "ground_truth": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill."}
{"query": "What is the default recompute ratio for tokens in the Llama-7B model?", "response": "The concept of a \"recompute ratio\" is not a standard term directly tied to the Llama-7B model or other large language models. However, it might be referring to techniques like **activation recomputation (also known as gradient checkpointing)**, which is used to save memory during training by recomputing intermediate activations during the backward pass instead of storing them in memory.\n\nFor models like LLaMA (Large Language Model Meta AI), including the 7B variant, the specific settings for recomputation (if used) depend on the implementation, framework, and configurations chosen during training or inference. Meta AI, the creators of LLaMA, has not published a fixed \"default recompute ratio\" as part of their public release.\n\nIf you are working with a specific implementation or training setup, the recompute ratio would typically be determined by the checkpointing strategy you configure, balancing memory usage and computation time. For example, some configurations might divide the model into segments and only store activations for certain segments, which indirectly sets the \"recompute ratio.\"\n\nIf you need precise details about this for LLaMA-7B in a specific codebase or training setup, it would be helpful to consult the associated documentation or configurations in the framework being used (e.g., PyTorch, Hugging Face, etc.).", "context": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill.", "ground_truth": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill."}
{"query": "How many lines of Python code were used to implement CacheBlend on top of vLLM?", "response": "CacheBlend, the extension of vLLM for efficient LLM inference with cache optimization, was implemented with **130 lines of Python code**. This concise implementation showcases the modularity and flexibility of vLLM, enabling researchers and developers to extend its capabilities with minimal effort.", "context": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill.", "ground_truth": "2405.16444v2.pdf:the KV caches\nsuch that: If KV cache is not inside the system and is recom-\nputed by the LLM engine in the runtime, we will move the\nKV cache into CPU by torch.cpu() and open a thread to\nwrite it back to disk in the background with torch.save().\nDuring fetch_kv, we go through through the hash tables to\nfetch KV cache for the fusor. The hash tables are kept in CPU\nfor their relatively small size (16MB for one million chunks).\n\n7 Evaluation\nOur key takeaways from the evaluation are:\n\n\u2022 TTFT reduction: Compared to full KV recompute,\nCacheBlend reduces TTFT by 2.2-3.3\u00d7 over several mod-\nels and tasks.\n\n\u2022 High quality:Compared with full KV reuse,CacheBlend\nimproves quality from 0.15 to 0.35 in F1-score and Rouge-L\nscore, while having no more than 0.01-0.03 quality drop\ncompared to full KV recompute and prefix caching.\n\n\u2022 Higher throughput: At the same TTFT, CacheBlend\ncan increase throughput by up to 5\u00d7 compared with full\nKV recompute and 3.3\u00d7 compared with prefix caching.\n\n7.1 Setup\n\nModels and hardware settings: We evaluate CacheBlend\non Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\na wide scale of open source models. Note that we apply 8-bit\nmodel quantization to Llama-70B and Yi-34B. We run our\nend-to-end experiments on Runpod GPUs [10] with 128 GB\nRAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\nsured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n7B and Yi-34B, and 2 GPUs to serve Llama-70B.\nDatasets: Our evaluation covers the following datasets.\n\u2022 2WikiMQA\n\n7 [27]: This dataset aims to test LLM\u2019s reasoning\nskills by requiring the model to read multiple paragraphs\nto answer a given question. We included 200 test cases,\nfollowing the dataset size of previous work [12].\n\n\u2022 Musique\n7 [51]: This is amulti-document question-answering\n\ndataset. It is designated to test LLM\u2019s multi-hop reason-\ning ability where one reasoning step critically relies on\ninformation from another and contains 150 test cases.\n\n2405.16444v2.pdf:of the existing work stores KV cache\nin volatile memory devices for guaranteed performance (e.g.,\nGPU HBM, CPU DRAM). While there are emerging research\ntrying to reuse high-speed NVME SSD for KV caches [21],\nCacheBlend is unique in pipelining loading with partial\nrecomputation and its extension to even slower object store.\n\nGeneral-purpose LLMserving systems: Numerous general-\npurpose LLM serving systems have been developed [11, 36,\n57, 60]. Orca [57] enables multiple requests to be processed\nin parallel with iteration-level scheduling. vLLM [36] further\nincreases the parallelsim through more efficent GPU mem-\nory management. CacheBlend is complementary to these\ngeneral-purpose LLM serving systems, empowering them\nwith context resuing capabilities.\n\nContext compression methods: Context compression\ntechniques [19, 31, 32, 43, 55, 58] can be complementary\nto CacheBlend. Some of these techniques [31, 32] shorten\nthe prompt length by prunining the unimportant tokens.\nCacheBlend is compatible with such methods in that it\ncan take different chunk lengths as shown in \u00a77.3. Another\nline of work [19, 43, 58] focus on dropping the unimportant\nKV vectors based on the attention matrix, which essentially\nreduce the KV cache size. CacheBlend can benifit from such\ntechniques by storing and loading less KV cache.\n\n9 Limitations\nWe acknowledge that our method (e.g., the insights in \u00a7 4.3)\nmay not apply to language models with architectures other\nthan transformer such as Mamba [26] and Griffin [17]. We\nhave not yet evaluated CacheBlend\u2019s performance on the\nlatest serving engines like Distserve[60] or StableGen [11].\nSince CacheBlend is able to reduce the costly prefill phase,\nwe believe combining CacheBlend with these new serving\nengines could potentially bring more savings. We believe our\nexperiments with the most recent open-source LLM engine\nvLLM that has been widely adopted suffices to verify our\nidea and demonstrate system performance. We leave such\n\n2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\nCached Knowledge Fusion\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\nJunchen Jiang\n\nUniversity of Chicago\n\nAbstract\nLarge language models (LLMs) often incorporate multiple\ntext chunks in their inputs to provide the necessary contexts.\nTo speed up the prefill of the long LLM inputs, one can\npre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input.\nHowever, the reused text chunks are not always the input\nprefix, and when they are not, their precomputed KV caches\ncannot be directly used since they ignore the text\u2019s cross-\nattention with the preceding text in the LLM input. Thus, the\nbenefits of reusing KV caches remain largely unrealized.\n\nThis paper tackles just one question: when an LLM input\ncontains multiple text chunks, how to quickly combine their\n\nprecomputed KV caches in order to achieve the same genera-\ntion quality as the expensive full prefill (i.e., without reusing\nKV cache)? We present CacheBlend, a scheme that reuses\nthe pre-computed KV caches, regardless prefix or not, and\nselectively recomputes the KV values of a small subset of tokens\n\nto partially update each reused KV cache. In the meantime,\nthe small extra delay for recomputing some tokens can be\npipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices\nwith more storage capacity while retrieving them without\n\nincreasing the inference delay. By comparing CacheBlend\nwith the state-of-the-art KV cache reusing schemes on three\nopen-source LLMs of various sizes and four popular bench-\nmark datasets of different tasks, we show that CacheBlend\nreduces time-to-first-token (TTFT) by 2.2\u20133.3\u00d7 and increases\nthe inference throughput by 2.8-5\u00d7, compared with full KV\nrecompute, without compromising generation quality or in-\ncurring more storage cost.\n\n1 Introduction\n\n2405.16444v2.pdf:is faster or equal to selective KV\nrecompute of one layer, the KV-loading delay should be able\nto hide the selective recompute delay, i.e., without incurring\nany extra delay on time-to-first-token (TTFT).\nTake the Llama-7B model and a 4K-long context, recom-\n\nputing 15% of the tokens (the default recompute ratio) only\ntakes 3ms per layer, while loading one layer\u2019s KV cache takes\n16 ms from an NVME SSD (\u00a77). In this case, KV loading can\nhide the delay for KV recompute on 15% of the tokens, i.e.,\nKV recompute incurs no extra delay. Recomputing more to-\nkens, which can slightly improve generation quality, may\nnot incur extra delay either, as long as the delay is below\n16 ms. On the contrary, with another model, Llama-70B, re-\ncomputing 15% of tokens takes 7 ms, but it only takes 4 ms\nto load one layer\u2019s KV from an NVME SSD. Here KV loading\ndoes not completely hide the recompute delay. In short, a\ncontroller is needed to intelligently pick the recompute ratio\nas well as where to store the KV cache (if applicable).\n\n5.1 Key Components\nTo realize the benefit of pipelining KV loading and recom-\npute, our system has three major components.\nLoading Controller: We face two design questions in prac-\ntice: First, given a storage device, how to choose a recompute\n\nratio (what fraction of tokens to recompute KV per layer) with-\n\nout incurring extra delay to time-to-first-token (TTFT)? Fig-\nure 10(a) illustrates an example that, if we select a recompute\n\n7\n\n\n\n: \u201cCan we use drones \nin agriculture?\u201d\n\nUser\nDrone \nChunk \n\n#1\n\nDrone \nChunk \n\n#2\n\nAgri\nChunk \n\n#1\n\nAgri\nChunk\n\n#2\n\nPotential New KV Cache\n\nLoading Controller\n\nCPU\n\nSSD\n\nSlower Disks\n\nKV Cache Store\nKV Cache FusorFused \n\nKV Cache\n\nKV Cache #1-4\n\n\u201cDrones   can \n\u2026\u201d\n\nCacheBlend System\n\nFigure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n\n2405.16444v2.pdf:a user of an LLM application\nsubmits a question, a list of relevant text chunks will be\nqueried. The loading controller then queries the KV cache\nmanager on whether the KV caches for those text chunks\nexist, and where they are stored. Next, the KV cache manager\nreturns this information back to the loading controller and\nthe controller computes the idealized selective recomputa-\ntion ratio, sends it to the fusor, and loads the KV caches into\na queue in GPU memory. The KV cache fusor continuously\nrecomputes the KV caches in the queue, until all layers are\nrecomputed. Lastly, the fused KV cache is input into the LLM\ninference engine, which generates the answer to the user\nquestion based on the KV cache.\n\n8\n\n\n\n6 Implementation\nWe implement CacheBlend on top of vLLM with about 3K\nlines of code in Python based on PyTorch v2.0.\nIntegrating Fusor into LLMserving engine: CacheBlend\nperforms the partial prefill process in a layer-wise manner\nthrough three interfaces:\n\u2022 fetch_kv(text, layer_id) -> KVCache: given a piece\nof text and a layer id, CacheBlend fetches the correspond-\ning KV cache from KV store into the GPU. Returns -1 if\nthe KV cache is not in the system.\n\n\u2022 prefill_layer(input_dict, KVCache) -> output_dict:\nCacheBlend takes in the input and KV cache of this layer\nand performs the partial prefill process for this particular\nlayer. The output is used as the input for the next layer.\n\n\u2022 synchronize(): CacheBlend requires synchronization\nbefore prefilling every layer to make sure the KV cache of\nthis layer has already been loaded into the GPU.\nWe implement these three interfaces inside vLLMs. For\n\nfetch_kv, we first calculate the hash of the text and search\nif it is inside the KV store system. If it is present, we call\ntorch.load() to load it into GPU memory if KV cache is\non disk or use torch.cuda() if the KV cache is inside CPU\nmemory. For prefill_layer, we implement this interface\non top of the original layer function in vLLM that performs\none layer of prefill."}
