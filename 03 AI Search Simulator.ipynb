{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Search\n",
    "### Using an AI Search Index for context data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the query for generating responses\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.identity import get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "# token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "result = search_client.get_document_count()\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are a very smart AI researcher helping users find answers to their questions. Your answers are short and simple.\n",
    "Answer the query using only the sources provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# Query is the question being asked. It's sent to the search engine and the chat model\n",
    "query=\"Cache Fusion\"\n",
    "\n",
    "# Search results are created by the search client\n",
    "# Search results are composed of the top 5 results and the fields selected from the search index\n",
    "# Search results include the top 5 matches to your query\n",
    "search_results = search_client.search(\n",
    "    search_text=query,\n",
    "    top=5,\n",
    "    select=\"title, chunk\",\n",
    "    vector_queries=[{\n",
    "        \"kind\": \"text\",\n",
    "        \"text\": query,\n",
    "        \"fields\": \"text_vector\",\n",
    "        \"k\": 3,\n",
    "    }],\n",
    ")\n",
    "\n",
    "sources_formatted = \"\\n\\n\".join([f'{document[\"title\"]}:{document[\"chunk\"]}' for document in search_results])\n",
    "\n",
    "print(sources_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Q&A Test (no evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": GROUNDED_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    ")\n",
    "\n",
    "# Here is the response from the chat model.\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        # azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    \n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    return message[\"content\"]\n",
    "\n",
    "\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    # call your endpoint or ai application here\n",
    "    response = call_to_your_ai_application(query)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Simulator to synthesize Q&A pairs from search query and index context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "simulator = Simulator(model_config=model_config)\n",
    "\n",
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=sources_formatted,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=1,\n",
    "    tasks=[\n",
    "        f\"I want to learn more about {query}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_file = Path(\"output.json\")\n",
    "with output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluations on the simulated data\n",
    "Here we will try to run GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator on the output data from the simulator.\n",
    "\n",
    "From the documentation we know that running those evaluators need the following data: `query`, `response`, `context`, `ground_truth`\n",
    "\n",
    "For simplicity's sake, we can use our source document `text` as both `context` and `ground_truth`. This step only evaluates the first user message and first response from your AI Application for each of the simulated conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_json_lines = \"\"\n",
    "for output in outputs:\n",
    "    query = None\n",
    "    response = None\n",
    "    context = sources_formatted\n",
    "    ground_truth = sources_formatted\n",
    "    for message in output[\"messages\"]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            query = message[\"content\"]\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            response = message[\"content\"]\n",
    "    if query and response:\n",
    "        eval_input_data_json_lines += (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"response\": response,\n",
    "                    \"context\": context,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the output in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_file = Path(\"eval_input_data.jsonl\")\n",
    "\n",
    "with eval_input_data_file.open(\"w\") as f:\n",
    "    f.write(eval_input_data_json_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation\n",
    "`QAEvaluator` is a composite evaluator which runs GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "Optionally set the azure_ai_project to upload the evaluation results to Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config=model_config)\n",
    "\n",
    "eval_output = evaluate(\n",
    "    data=str(eval_input_data_file),\n",
    "    evaluators={\"QAEvaluator\": qa_evaluator},\n",
    "    evaluator_config={\n",
    "        \"QAEvaluator\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,  # optional to store the evaluation results in Azure AI Studio\n",
    "    output_path=\"./myevalresults.json\",  # optional to store the evaluation results in a file\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
