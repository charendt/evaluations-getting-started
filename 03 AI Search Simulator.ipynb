{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Search\n",
    "### Using an AI Search Index for context data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2405.16444v2.pdf:the KV caches\n",
      "such that: If KV cache is not inside the system and is recom-\n",
      "puted by the LLM engine in the runtime, we will move the\n",
      "KV cache into CPU by torch.cpu() and open a thread to\n",
      "write it back to disk in the background with torch.save().\n",
      "During fetch_kv, we go through through the hash tables to\n",
      "fetch KV cache for the fusor. The hash tables are kept in CPU\n",
      "for their relatively small size (16MB for one million chunks).\n",
      "\n",
      "7 Evaluation\n",
      "Our key takeaways from the evaluation are:\n",
      "\n",
      "• TTFT reduction: Compared to full KV recompute,\n",
      "CacheBlend reduces TTFT by 2.2-3.3× over several mod-\n",
      "els and tasks.\n",
      "\n",
      "• High quality:Compared with full KV reuse,CacheBlend\n",
      "improves quality from 0.15 to 0.35 in F1-score and Rouge-L\n",
      "score, while having no more than 0.01-0.03 quality drop\n",
      "compared to full KV recompute and prefix caching.\n",
      "\n",
      "• Higher throughput: At the same TTFT, CacheBlend\n",
      "can increase throughput by up to 5× compared with full\n",
      "KV recompute and 3.3× compared with prefix caching.\n",
      "\n",
      "7.1 Setup\n",
      "\n",
      "Models and hardware settings: We evaluate CacheBlend\n",
      "on Mistral-7B[30], Yi-34B[56] and Llama-70B[2] to represent\n",
      "a wide scale of open source models. Note that we apply 8-bit\n",
      "model quantization to Llama-70B and Yi-34B. We run our\n",
      "end-to-end experiments on Runpod GPUs [10] with 128 GB\n",
      "RAM, 2 Nvidia A40 GPUs, and 1TB NVME SSD whose mea-\n",
      "sured throughput is 4.8 GB/s. We use 1 GPU to serve Mistral-\n",
      "7B and Yi-34B, and 2 GPUs to serve Llama-70B.\n",
      "Datasets: Our evaluation covers the following datasets.\n",
      "• 2WikiMQA\n",
      "\n",
      "7 [27]: This dataset aims to test LLM’s reasoning\n",
      "skills by requiring the model to read multiple paragraphs\n",
      "to answer a given question. We included 200 test cases,\n",
      "following the dataset size of previous work [12].\n",
      "\n",
      "• Musique\n",
      "7 [51]: This is amulti-document question-answering\n",
      "\n",
      "dataset. It is designated to test LLM’s multi-hop reason-\n",
      "ing ability where one reasoning step critically relies on\n",
      "information from another and contains 150 test cases.\n",
      "\n",
      "2405.16444v2.pdf:of the existing work stores KV cache\n",
      "in volatile memory devices for guaranteed performance (e.g.,\n",
      "GPU HBM, CPU DRAM). While there are emerging research\n",
      "trying to reuse high-speed NVME SSD for KV caches [21],\n",
      "CacheBlend is unique in pipelining loading with partial\n",
      "recomputation and its extension to even slower object store.\n",
      "\n",
      "General-purpose LLMserving systems: Numerous general-\n",
      "purpose LLM serving systems have been developed [11, 36,\n",
      "57, 60]. Orca [57] enables multiple requests to be processed\n",
      "in parallel with iteration-level scheduling. vLLM [36] further\n",
      "increases the parallelsim through more efficent GPU mem-\n",
      "ory management. CacheBlend is complementary to these\n",
      "general-purpose LLM serving systems, empowering them\n",
      "with context resuing capabilities.\n",
      "\n",
      "Context compression methods: Context compression\n",
      "techniques [19, 31, 32, 43, 55, 58] can be complementary\n",
      "to CacheBlend. Some of these techniques [31, 32] shorten\n",
      "the prompt length by prunining the unimportant tokens.\n",
      "CacheBlend is compatible with such methods in that it\n",
      "can take different chunk lengths as shown in §7.3. Another\n",
      "line of work [19, 43, 58] focus on dropping the unimportant\n",
      "KV vectors based on the attention matrix, which essentially\n",
      "reduce the KV cache size. CacheBlend can benifit from such\n",
      "techniques by storing and loading less KV cache.\n",
      "\n",
      "9 Limitations\n",
      "We acknowledge that our method (e.g., the insights in § 4.3)\n",
      "may not apply to language models with architectures other\n",
      "than transformer such as Mamba [26] and Griffin [17]. We\n",
      "have not yet evaluated CacheBlend’s performance on the\n",
      "latest serving engines like Distserve[60] or StableGen [11].\n",
      "Since CacheBlend is able to reduce the costly prefill phase,\n",
      "we believe combining CacheBlend with these new serving\n",
      "engines could potentially bring more savings. We believe our\n",
      "experiments with the most recent open-source LLM engine\n",
      "vLLM that has been widely adopted suffices to verify our\n",
      "idea and demonstrate system performance. We leave such\n",
      "\n",
      "2405.16444v2.pdf:CacheBlend: Fast Large Language Model Serving for RAG with\n",
      "Cached Knowledge Fusion\n",
      "\n",
      "Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,\n",
      "Junchen Jiang\n",
      "\n",
      "University of Chicago\n",
      "\n",
      "Abstract\n",
      "Large language models (LLMs) often incorporate multiple\n",
      "text chunks in their inputs to provide the necessary contexts.\n",
      "To speed up the prefill of the long LLM inputs, one can\n",
      "pre-compute the KV cache of a text and re-use the KV cache\n",
      "when the context is reused as the prefix of another LLM input.\n",
      "However, the reused text chunks are not always the input\n",
      "prefix, and when they are not, their precomputed KV caches\n",
      "cannot be directly used since they ignore the text’s cross-\n",
      "attention with the preceding text in the LLM input. Thus, the\n",
      "benefits of reusing KV caches remain largely unrealized.\n",
      "\n",
      "This paper tackles just one question: when an LLM input\n",
      "contains multiple text chunks, how to quickly combine their\n",
      "\n",
      "precomputed KV caches in order to achieve the same genera-\n",
      "tion quality as the expensive full prefill (i.e., without reusing\n",
      "KV cache)? We present CacheBlend, a scheme that reuses\n",
      "the pre-computed KV caches, regardless prefix or not, and\n",
      "selectively recomputes the KV values of a small subset of tokens\n",
      "\n",
      "to partially update each reused KV cache. In the meantime,\n",
      "the small extra delay for recomputing some tokens can be\n",
      "pipelined with the retrieval of KV caches within the same job,\n",
      "allowing CacheBlend to store KV caches in slower devices\n",
      "with more storage capacity while retrieving them without\n",
      "\n",
      "increasing the inference delay. By comparing CacheBlend\n",
      "with the state-of-the-art KV cache reusing schemes on three\n",
      "open-source LLMs of various sizes and four popular bench-\n",
      "mark datasets of different tasks, we show that CacheBlend\n",
      "reduces time-to-first-token (TTFT) by 2.2–3.3× and increases\n",
      "the inference throughput by 2.8-5×, compared with full KV\n",
      "recompute, without compromising generation quality or in-\n",
      "curring more storage cost.\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "2405.16444v2.pdf:is faster or equal to selective KV\n",
      "recompute of one layer, the KV-loading delay should be able\n",
      "to hide the selective recompute delay, i.e., without incurring\n",
      "any extra delay on time-to-first-token (TTFT).\n",
      "Take the Llama-7B model and a 4K-long context, recom-\n",
      "\n",
      "puting 15% of the tokens (the default recompute ratio) only\n",
      "takes 3ms per layer, while loading one layer’s KV cache takes\n",
      "16 ms from an NVME SSD (§7). In this case, KV loading can\n",
      "hide the delay for KV recompute on 15% of the tokens, i.e.,\n",
      "KV recompute incurs no extra delay. Recomputing more to-\n",
      "kens, which can slightly improve generation quality, may\n",
      "not incur extra delay either, as long as the delay is below\n",
      "16 ms. On the contrary, with another model, Llama-70B, re-\n",
      "computing 15% of tokens takes 7 ms, but it only takes 4 ms\n",
      "to load one layer’s KV from an NVME SSD. Here KV loading\n",
      "does not completely hide the recompute delay. In short, a\n",
      "controller is needed to intelligently pick the recompute ratio\n",
      "as well as where to store the KV cache (if applicable).\n",
      "\n",
      "5.1 Key Components\n",
      "To realize the benefit of pipelining KV loading and recom-\n",
      "pute, our system has three major components.\n",
      "Loading Controller: We face two design questions in prac-\n",
      "tice: First, given a storage device, how to choose a recompute\n",
      "\n",
      "ratio (what fraction of tokens to recompute KV per layer) with-\n",
      "\n",
      "out incurring extra delay to time-to-first-token (TTFT)? Fig-\n",
      "ure 10(a) illustrates an example that, if we select a recompute\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      ": “Can we use drones \n",
      "in agriculture?”\n",
      "\n",
      "User\n",
      "Drone \n",
      "Chunk \n",
      "\n",
      "#1\n",
      "\n",
      "Drone \n",
      "Chunk \n",
      "\n",
      "#2\n",
      "\n",
      "Agri\n",
      "Chunk \n",
      "\n",
      "#1\n",
      "\n",
      "Agri\n",
      "Chunk\n",
      "\n",
      "#2\n",
      "\n",
      "Potential New KV Cache\n",
      "\n",
      "Loading Controller\n",
      "\n",
      "CPU\n",
      "\n",
      "SSD\n",
      "\n",
      "Slower Disks\n",
      "\n",
      "KV Cache Store\n",
      "KV Cache FusorFused \n",
      "\n",
      "KV Cache\n",
      "\n",
      "KV Cache #1-4\n",
      "\n",
      "“Drones   can \n",
      "…”\n",
      "\n",
      "CacheBlend System\n",
      "\n",
      "Figure 11. CacheBlend system (green stared) in light of LLM context augmented generation for a single request. CacheBlend\n",
      "\n",
      "2405.16444v2.pdf:a user of an LLM application\n",
      "submits a question, a list of relevant text chunks will be\n",
      "queried. The loading controller then queries the KV cache\n",
      "manager on whether the KV caches for those text chunks\n",
      "exist, and where they are stored. Next, the KV cache manager\n",
      "returns this information back to the loading controller and\n",
      "the controller computes the idealized selective recomputa-\n",
      "tion ratio, sends it to the fusor, and loads the KV caches into\n",
      "a queue in GPU memory. The KV cache fusor continuously\n",
      "recomputes the KV caches in the queue, until all layers are\n",
      "recomputed. Lastly, the fused KV cache is input into the LLM\n",
      "inference engine, which generates the answer to the user\n",
      "question based on the KV cache.\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "6 Implementation\n",
      "We implement CacheBlend on top of vLLM with about 3K\n",
      "lines of code in Python based on PyTorch v2.0.\n",
      "Integrating Fusor into LLMserving engine: CacheBlend\n",
      "performs the partial prefill process in a layer-wise manner\n",
      "through three interfaces:\n",
      "• fetch_kv(text, layer_id) -> KVCache: given a piece\n",
      "of text and a layer id, CacheBlend fetches the correspond-\n",
      "ing KV cache from KV store into the GPU. Returns -1 if\n",
      "the KV cache is not in the system.\n",
      "\n",
      "• prefill_layer(input_dict, KVCache) -> output_dict:\n",
      "CacheBlend takes in the input and KV cache of this layer\n",
      "and performs the partial prefill process for this particular\n",
      "layer. The output is used as the input for the next layer.\n",
      "\n",
      "• synchronize(): CacheBlend requires synchronization\n",
      "before prefilling every layer to make sure the KV cache of\n",
      "this layer has already been loaded into the GPU.\n",
      "We implement these three interfaces inside vLLMs. For\n",
      "\n",
      "fetch_kv, we first calculate the hash of the text and search\n",
      "if it is inside the KV store system. If it is present, we call\n",
      "torch.load() to load it into GPU memory if KV cache is\n",
      "on disk or use torch.cuda() if the KV cache is inside CPU\n",
      "memory. For prefill_layer, we implement this interface\n",
      "on top of the original layer function in vLLM that performs\n",
      "one layer of prefill.\n"
     ]
    }
   ],
   "source": [
    "# Set up the query for generating responses\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.identity import get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "# token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "result = search_client.get_document_count()\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are a very smart AI researcher helping users find answers to their questions. Your answers are short and simple.\n",
    "Answer the query using only the sources provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# Query is the question being asked. It's sent to the search engine and the chat model\n",
    "query=\"Cache Fusion\"\n",
    "\n",
    "# Search results are created by the search client\n",
    "# Search results are composed of the top 5 results and the fields selected from the search index\n",
    "# Search results include the top 5 matches to your query\n",
    "search_results = search_client.search(\n",
    "    search_text=query,\n",
    "    top=5,\n",
    "    select=\"title, chunk\",\n",
    "    vector_queries=[{\n",
    "        \"kind\": \"text\",\n",
    "        \"text\": query,\n",
    "        \"fields\": \"text_vector\",\n",
    "        \"k\": 3,\n",
    "    }],\n",
    ")\n",
    "\n",
    "sources_formatted = \"\\n\\n\".join([f'{document[\"title\"]}:{document[\"chunk\"]}' for document in search_results])\n",
    "\n",
    "print(sources_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Q&A Test (no evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Cache Fusion in CacheBlend**:\n",
      "  - CacheBlend combines precomputed Key-Value (KV) caches from multiple text chunks to speed up large language model (LLM) inferences without fully recomputing KV caches. \n",
      "  - The KV cache fusor integrates precomputed KV caches while selectively recomputing KV values for partial updates.\n",
      "  - This approach reduces time-to-first-token (TTFT) by 2.2–3.3× and increases throughput by up to 5× compared to full KV recompute.\n",
      "\n",
      "- **KV Cache Handling**:\n",
      "  - KV caches are stored in slower storage devices (e.g., NVMe SSDs) and are brought into memory when needed.\n",
      "  - During LLM input processing, the fusor recomputes necessary portions of the KV cache while loading precomputed segments, maintaining efficiency.\n",
      "\n",
      "- **System Workflow**:\n",
      "  - A loading controller queries a KV cache manager to determine the storage location of KV caches and computes the selective recompute ratio.\n",
      "  - KV caches are placed in GPU memory in a queue for computation and fusion.\n",
      "  - After KV cache fusion, the final fused cache is passed to the LLM inference engine for generating output. \n",
      "\n",
      "- **Hardware Support**:\n",
      "  - CacheBlend has been tested using GPUs, such as Nvidia A40, and NVMe SSD storage for efficient KV cache storage and retrieval.\n",
      "\n",
      "- **Implementation**:\n",
      "  - Written in Python using PyTorch, integrated into vLLM LLM-serving engines.\n",
      "  - Designed interfaces like `fetch_kv`, `prefill_layer`, and `synchronize` manage KV cache retrieval and processing.\n",
      "\n",
      "- **Limitations**:\n",
      "  - CacheBlend is designed for transformer-based architectures (e.g., Llama, Mistral) and is not tested on non-transformer models like Mamba or Griffin.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": GROUNDED_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    ")\n",
    "\n",
    "# Here is the response from the chat model.\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        # azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    \n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    return message[\"content\"]\n",
    "\n",
    "\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    # call your endpoint or ai application here\n",
    "    response = call_to_your_ai_application(query)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Simulator to synthesize Q&A pairs from search query and index context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "judge_model = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class Simulator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "c:\\Users\\charendt\\code\\evaluations-getting-started\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\simulator\\_simulator.py:151: UserWarning: You have specified 'num_queries' > len('tasks') (4 > 1). All tasks will be used for generation and the remaining 3 lines will be simulated in task-free mode\n",
      "  warnings.warn(\n",
      "Generating: 100%|████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.02s/message]\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "simulator = Simulator(model_config=judge_model)\n",
    "\n",
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=sources_formatted,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=1,\n",
    "    tasks=[\n",
    "        f\"I want to learn more about {query}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "output_file = Path(\"output.json\")\n",
    "with output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluations on the simulated data\n",
    "Here we will try to run GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator on the output data from the simulator.\n",
    "\n",
    "From the documentation we know that running those evaluators need the following data: `query`, `response`, `context`, `ground_truth`\n",
    "\n",
    "For simplicity's sake, we can use our source document `text` as both `context` and `ground_truth`. This step only evaluates the first user message and first response from your AI Application for each of the simulated conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_json_lines = \"\"\n",
    "for output in outputs:\n",
    "    query = None\n",
    "    response = None\n",
    "    context = sources_formatted\n",
    "    ground_truth = sources_formatted\n",
    "    for message in output[\"messages\"]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            query = message[\"content\"]\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            response = message[\"content\"]\n",
    "    if query and response:\n",
    "        eval_input_data_json_lines += (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"response\": response,\n",
    "                    \"context\": context,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the output in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_data_file = Path(\"eval_input_data.jsonl\")\n",
    "\n",
    "with eval_input_data_file.open(\"w\") as f:\n",
    "    f.write(eval_input_data_json_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation\n",
    "`QAEvaluator` is a composite evaluator which runs GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "Optionally set the azure_ai_project to upload the evaluation results to Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.getenv(\"AZURE_PROJECT_NAME\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-24 18:26:50 +0100][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-02-24 18:26:50 +0100][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ie0kadbq_20250224_182650_474617, log path: C:\\Users\\charendt\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ie0kadbq_20250224_182650_474617\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:26:50 +0100   23152 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-02-24 18:27:13 +0100   23152 execution.bulk     INFO     Finished 4 / 4 lines.\n",
      "2025-02-24 18:27:13 +0100   23152 execution.bulk     INFO     Average execution time for completed lines: 5.58 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ie0kadbq_20250224_182650_474617\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-02-24 18:26:50.472621+01:00\"\n",
      "Duration: \"0:00:22.852234\"\n",
      "Output path: \"C:\\Users\\charendt\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ie0kadbq_20250224_182650_474617\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"QAEvaluator\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:22.852234\",\n",
      "        \"completed_lines\": 4,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\charendt\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ie0kadbq_20250224_182650_474617\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Evaluation results saved to \"C:\\Users\\charendt\\code\\evaluations-getting-started\\myevalresults.json\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config=judge_model)\n",
    "\n",
    "eval_output = evaluate(\n",
    "    data=str(eval_input_data_file),\n",
    "    evaluators={\"QAEvaluator\": qa_evaluator},\n",
    "    evaluator_config={\n",
    "        \"QAEvaluator\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,  # optional to store the evaluation results in Azure AI Studio\n",
    "    output_path=\"./myevalresults.json\",  # optional to store the evaluation results in a file\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
